# -*- coding: utf-8 -*-
"""Log_ML_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XOcUBJuew-f-TLs533XrU5zx3Q-OOssv
"""

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import LinearRegression
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score


from google.colab import drive
drive.mount('/content/drive')

"""##Performance datapreprocessing

###Attack
"""

file_path = 'system_logs_labeled.csv'
df = pd.read_csv(file_path)

df['Timestamp'] = pd.to_datetime(df['Timestamp'])

df_diff = df.copy()

columns_to_diff = ['Used Memory (MB)', 'CPU Load 1 min', 'CPU Load 5 min', 'CPU Load 15 min',
                   'Disk Usage (%)', 'TX (MB)', 'RX (MB)']

for col in columns_to_diff:
    df_diff[col + ' Diff'] = df[col].diff()

df_diff = df_diff.dropna().reset_index(drop=True)

df_diff['CPU Load 1 min Actual'] = df_diff['CPU Load 1 min']
df_diff['CPU Load 5 min Actual'] = df_diff['CPU Load 5 min']
df_diff['CPU Load 15 min Actual'] = df_diff['CPU Load 15 min']

columns_to_keep = ['Timestamp'] + [col + ' Diff' for col in columns_to_diff] + [
    'CPU Load 1 min Actual', 'CPU Load 5 min Actual', 'CPU Load 15 min Actual',
    'Open Ports', 'Label', 'Attack Technique'
]

df_final = df_diff[columns_to_keep]

df_final.loc[
    (df_final['Timestamp'] >= '2024-08-24 23:52:00') &
    (df_final['Timestamp'] <= '2024-08-25 00:01:51'),
    ['Label', 'Attack Technique']
] = [1, 10]

output_file_path = 'labeled_system_logs_diff.csv'
df_final.to_csv(output_file_path, index=False)

print(f"Processed logs with selected features have been saved to {output_file_path}")

file_path = 'datalogger_labeled.csv'
df = pd.read_csv(file_path)

df['Timestamp'] = pd.to_datetime(
    df['Arp.Plc.Eclr/code.RTC_DT.YEAR'].astype(str) + '-' +
    df['Arp.Plc.Eclr/code.RTC_DT.MONTH'].astype(str).str.zfill(2) + '-' +
    df['Arp.Plc.Eclr/code.RTC_DT.DAY'].astype(str).str.zfill(2) + ' ' +
    df['Arp.Plc.Eclr/code.RTC_DT.HOURS'].astype(str).str.zfill(2) + ':' +
    df['Arp.Plc.Eclr/code.RTC_DT.MINUTES'].astype(str).str.zfill(2) + ':' +
    df['Arp.Plc.Eclr/code.RTC_DT.SECONDS'].astype(str).str.zfill(2)
)

columns_to_drop = [
    'Arp.Plc.Eclr/code.AXIO_ACT', 'Arp.Plc.Eclr/code.AXIO_BUS',
    'Arp.Plc.Eclr/code.AXIO_PF', 'Arp.Plc.Eclr/code.AXIO_PW',
    'Arp.Plc.Eclr/code.AXIO_RDY', 'Arp.Plc.Eclr/code.AXIO_RUN',
    'Arp.Plc.Eclr/code.AXIO_SYSFAIL', 'ConsistentDataSeries',
    'Arp.Plc.Eclr/code.RTC_DT.DAY', 'Arp.Plc.Eclr/code.RTC_DT.HOURS',
    'Arp.Plc.Eclr/code.RTC_DT.MINUTES', 'Arp.Plc.Eclr/code.RTC_DT.MONTH',
    'Arp.Plc.Eclr/code.RTC_DT.SECONDS', 'Arp.Plc.Eclr/code.RTC_DT.YEAR',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_TOTAL', 'RecordAttribute'
]

df = df.drop(columns=columns_to_drop)

columns_to_average = [
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[1].TASK_INFOS[1].LAST_ACTIVATION_DELAY',
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[1].TASK_INFOS[1].LAST_EXEC_DURATION',
    'Arp.Plc.Eclr/code.IP_ACTIVE_DT',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_FREE',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_USAGE',
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[2].TASK_INFOS[1].LAST_ACTIVATION_DELAY',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_USED',
    'Arp.Plc.Eclr/code.cpu_1',
    'Arp.Plc.Eclr/code.cpu_2',
    'Arp.Plc.Eclr/code.temp',
    'Total_Exec_Duration',
    'Total_Activation_delay',
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[2].TASK_INFOS[1].LAST_EXEC_DURATION'
]

def process_in_chunks(df, chunk_size=50):
    processed_rows = []

    for start in range(0, len(df), chunk_size):
        chunk = df.iloc[start:start + chunk_size]

        if len(chunk) < chunk_size:
            continue

        averaged_chunk = chunk[columns_to_average].mean().to_dict()

        label = chunk.iloc[-1]['Label']
        attack_technique = chunk.iloc[-1]['Attack Technique']

        timestamp = chunk.iloc[-1]['Timestamp']

        new_row = {**averaged_chunk, 'Label': label, 'Attack Technique': attack_technique, 'Timestamp': timestamp}

        processed_rows.append(new_row)

    return pd.DataFrame(processed_rows)

df_processed = process_in_chunks(df, chunk_size=50)

df_processed.loc[
    (df_processed['Timestamp'] >= '2024-08-24 23:51:00') &
    (df_processed['Timestamp'] <= '2024-08-25 00:01:51'),
    ['Label', 'Attack Technique']
] = [1, 10]


output_file_path = 'labeled_dl_logs_diff.csv'
df_processed.to_csv(output_file_path, index=False)

print(f"Processed data has been saved to {output_file_path}")

df1 = pd.read_csv('labeled_system_logs_diff.csv')
df2 = pd.read_csv('labeled_dl_logs_diff.csv')

df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])
df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])

df1.sort_values('Timestamp', inplace=True)
df2.sort_values('Timestamp', inplace=True)

df_combined = pd.merge_asof(df1, df2, on='Timestamp', direction='nearest', suffixes=('', '_dl'))

df_combined['Timestamp'] = df1['Timestamp'].reset_index(drop=True)

df_combined = df_combined.drop(columns=['Label', 'Attack Technique'])

df_combined.rename(columns={'Label_dl': 'Label', 'Attack Technique_dl': 'Attack Technique'}, inplace=True)

df_combined['Timestamp'] = df_combined['Timestamp'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds().cumsum()


output_file_path = 'combined_data.csv'
df_combined.to_csv(output_file_path, index=False)

print(f"Combined data has been saved to {output_file_path}")

"""### Normal Operation

"""

file_path = 'system_label.csv'
df = pd.read_csv(file_path)

df['Timestamp'] = pd.to_datetime(df['Timestamp'])

df_diff = df.copy()

columns_to_diff = ['Used Memory (MB)', 'CPU Load 1 min', 'CPU Load 5 min', 'CPU Load 15 min',
                   'Disk Usage (%)', 'TX (MB)', 'RX (MB)']

for col in columns_to_diff:
    df_diff[col + ' Diff'] = df[col].diff()

df_diff = df_diff.dropna().reset_index(drop=True)

df_diff['CPU Load 1 min Actual'] = df_diff['CPU Load 1 min']
df_diff['CPU Load 5 min Actual'] = df_diff['CPU Load 5 min']
df_diff['CPU Load 15 min Actual'] = df_diff['CPU Load 15 min']

columns_to_keep = ['Timestamp'] + [col + ' Diff' for col in columns_to_diff] + [
    'CPU Load 1 min Actual', 'CPU Load 5 min Actual', 'CPU Load 15 min Actual',
    'Open Ports', 'Label', 'Attack Technique'
]

df_final = df_diff[columns_to_keep]


output_file_path = 'labeled_system_logs_diff.csv'
df_final.to_csv(output_file_path, index=False)

print(f"Processed logs with selected features have been saved to {output_file_path}")

file_path = 'datalogger_labeled.csv'
df = pd.read_csv(file_path)

df['Timestamp'] = pd.to_datetime(
    df['Arp.Plc.Eclr/code.RTC_DT.YEAR'].astype(str) + '-' +
    df['Arp.Plc.Eclr/code.RTC_DT.MONTH'].astype(str).str.zfill(2) + '-' +
    df['Arp.Plc.Eclr/code.RTC_DT.DAY'].astype(str).str.zfill(2) + ' ' +
    df['Arp.Plc.Eclr/code.RTC_DT.HOURS'].astype(str).str.zfill(2) + ':' +
    df['Arp.Plc.Eclr/code.RTC_DT.MINUTES'].astype(str).str.zfill(2) + ':' +
    df['Arp.Plc.Eclr/code.RTC_DT.SECONDS'].astype(str).str.zfill(2)
)

columns_to_drop = [
    'Arp.Plc.Eclr/code.AXIO_ACT', 'Arp.Plc.Eclr/code.AXIO_BUS',
    'Arp.Plc.Eclr/code.AXIO_PF', 'Arp.Plc.Eclr/code.AXIO_PW',
    'Arp.Plc.Eclr/code.AXIO_RDY', 'Arp.Plc.Eclr/code.AXIO_RUN',
    'Arp.Plc.Eclr/code.AXIO_SYSFAIL', 'ConsistentDataSeries',
    'Arp.Plc.Eclr/code.RTC_DT.DAY', 'Arp.Plc.Eclr/code.RTC_DT.HOURS',
    'Arp.Plc.Eclr/code.RTC_DT.MINUTES', 'Arp.Plc.Eclr/code.RTC_DT.MONTH',
    'Arp.Plc.Eclr/code.RTC_DT.SECONDS', 'Arp.Plc.Eclr/code.RTC_DT.YEAR',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_TOTAL', 'RecordAttribute'
]

df = df.drop(columns=columns_to_drop)

columns_to_average = [
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[1].TASK_INFOS[1].LAST_ACTIVATION_DELAY',
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[1].TASK_INFOS[1].LAST_EXEC_DURATION',
    'Arp.Plc.Eclr/code.IP_ACTIVE_DT',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_FREE',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_USAGE',
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[2].TASK_INFOS[1].LAST_ACTIVATION_DELAY',
    'Arp.Plc.Eclr/code.USER_PART_DT.MEM_USED',
    'Arp.Plc.Eclr/code.cpu_1',
    'Arp.Plc.Eclr/code.cpu_2',
    'Arp.Plc.Eclr/code.temp',
    'Total_Exec_Duration',
    'Total_Activation_delay',
    'Arp.Plc.Eclr/code.ESM_DATA1.ESM_INFOS[2].TASK_INFOS[1].LAST_EXEC_DURATION'
]

def process_in_chunks(df, chunk_size=50):
    processed_rows = []

    for start in range(0, len(df), chunk_size):
        chunk = df.iloc[start:start + chunk_size]

        if len(chunk) < chunk_size:
            continue

        averaged_chunk = chunk[columns_to_average].mean().to_dict()

        label = 0
        attack_technique = 0

        timestamp = chunk.iloc[-1]['Timestamp']

        new_row = {**averaged_chunk, 'Label': label, 'Attack Technique': attack_technique, 'Timestamp': timestamp}

        processed_rows.append(new_row)

    return pd.DataFrame(processed_rows)

df_processed = process_in_chunks(df, chunk_size=50)

output_file_path = 'labeled_dl_logs_diff.csv'
df_processed.to_csv(output_file_path, index=False)

print(f"Processed data has been saved to {output_file_path}")

df1 = pd.read_csv('/content/drive/MyDrive/Master/R2_Machine_Learning/Normal/ml/labeled_system_logs_diff.csv')
df2 = pd.read_csv('/content/drive/MyDrive/Master/R2_Machine_Learning/Normal/ml/labeled_dl_logs_diff.csv')

df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])
df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])

df1.sort_values('Timestamp', inplace=True)
df2.sort_values('Timestamp', inplace=True)

df_combined = pd.merge_asof(df1, df2, on='Timestamp', direction='nearest', suffixes=('', '_dl'))

df_combined['Timestamp'] = df1['Timestamp'].reset_index(drop=True)

df_combined = df_combined.drop(columns=['Label', 'Attack Technique'])

df_combined.rename(columns={'Label_dl': 'Label', 'Attack Technique_dl': 'Attack Technique'}, inplace=True)

df_combined['Timestamp'] = df_combined['Timestamp'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds().cumsum()

df_combined = df_combined[24:]

output_file_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Normal/ml/combined_data.csv'
df_combined.to_csv(output_file_path, index=False)

print(f"Combined data has been saved to {output_file_path}")

"""##Merg of Normal and Attack"""

df1 = pd.read_csv('/content/drive/MyDrive/Master/R2_Machine_Learning/Normal/ml/combined_data.csv')
df2 = pd.read_csv('/content/drive/MyDrive/Master/R2_Machine_Learning/Attack/ml/Perf_dataset.csv')

df_combined = pd.concat([df1, df2], ignore_index=True)

output_file_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/perf_dataset.csv'
df_combined.to_csv(output_file_path, index=False)

print(f"Combined data has been saved to {output_file_path}")

"""###Preparing Event merge"""

file_paths = {
    "security_labeled.csv": 1,
    "Output_labeled.csv": 1,
    "default_labeled.csv": 1, #"all_network_logs_labeled.csv": 2,   Add it if you want to use the network-related logs
    "all_logs_labeled.csv": 2
}

base_path = "Labeled"

# Initialize an empty list to hold all data
all_data = []

# Loop through each file and load it with the appropriate data-type label
for file_name, data_type in file_paths.items():
    # Load the CSV file
    df = pd.read_csv(f"{base_path}/{file_name}")

    # Add the data-type column
    df['data-type'] = data_type

    # Append the data to the list
    all_data.append(df)

# Concatenate all data into a single DataFrame
df_combined = pd.concat(all_data, ignore_index=True)

# Ensure that only 'Message', 'Label', 'Attack Technique', and 'data-type' columns are retained
columns_to_keep = ['Message', 'Label', 'Attack Technique', 'data-type']

# If 'Output_label.csv' has an additional 'Label' column, merge it correctly
if 'Label' in df_combined.columns:
    df_combined['Label'] = df_combined['Label'].fillna(0).astype(int)  # Ensure Label column is consistent

df_combined = df_combined[columns_to_keep]

# Save the combined DataFrame to a new CSV file
output_file_path = "combined_events.csv"
df_combined.to_csv(output_file_path, index=False)

print(f"Combined data has been saved to {output_file_path}")

file_paths = {
    "security_labeled.csv": 1,
    "output_label.csv": 1,
    "default_labeled.csv": 1,
    "event_logs_label.csv": 2
}

base_path = "Labeled"

all_data = []

for file_name, data_type in file_paths.items():

    df = pd.read_csv(f"{base_path}/{file_name}")


    df['data-type'] = data_type


    all_data.append(df)


df_combined = pd.concat(all_data, ignore_index=True)


columns_to_keep = ['Message', 'Label', 'Attack Technique', 'data-type']


if 'Label' in df_combined.columns:
    df_combined['Label'] = df_combined['Label'].fillna(0).astype(int)

df_combined = df_combined[columns_to_keep]


output_file_path = "/content/drive/MyDrive/Master/R2_Machine_Learning/Normal/ml/combined_events.csv"
df_combined.to_csv(output_file_path, index=False)

print(f"Combined data has been saved to {output_file_path}")

df_normal = pd.read_csv("/content/drive/MyDrive/Master/R2_Machine_Learning/Normal/ml/combined_events.csv")
df_attack = pd.read_csv("/content/drive/MyDrive/Master/R2_Machine_Learning/Attack/experiment/combined_events.csv")


columns_to_keep = ['Message', 'Label', 'Attack Technique', 'data-type']
df_normal = df_normal[columns_to_keep]
df_attack = df_attack[columns_to_keep]


df_final_combined = pd.concat([df_combined, df_normal, df_attack], ignore_index=True)


final_output_path = "/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/event_logs.csv"
df_final_combined.to_csv(final_output_path, index=False)

print(f"Final combined data has been saved to {final_output_path}")

"""##Event Datapreprocessing"""

file_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/event_logs.csv'
df = pd.read_csv(file_path)


keywords = [
    'axcf2152 nginx', 'cron_monit.sh', 'bash_monitor.sh', 'firewall_monit.sh', 'c2_monit.sh', 'monitor_plc_log.py',
    'bash_access.sh', 'system_log.sh', 'file_monit.sh', 'linpease.sh', 'fibofiled.sh', '"pam_unix(crond:session)"',
    'cron', 'private_file', '"axcf2152 syslog-ng"', '"Accepted password"', 'CMD:', 'admin', 'root', 'engineer',
    'attacker', 'firewall', 'php_app', 'backup.php', 'private_file', 'Users.config', 'disconnect', 'authentic',
    'Connection', 'retries', 'mycronjob', 'New password', 'Passwordchange', 'plcnext_shadow.sqlite', 'plcnext_users',
    'plcnext_firewall_rules', 'Plc state', '"Installation Mode"', '192.168.137.246', '192.168.137.50', '192.168.137.10',
    '192.168.137.1', '192.168.137.12',
    'StatusChanged', 'stopped', 'Plc state', 'security', 'Firmware', 'changed',
    'delayed', 'expired', 'DB_name', 'Sec_log', 'Severity', 'Sender', 'UserAdded',
    'technicker', 'sessions', '"Security Token"', 'Ethernet', 'Deactivated',
    'PasswordPolicy', 'UserRemoved', 'UserAuthentication', 'Startup'
]


extracted_data = []


def extract_matching_text(message):
    extracted = []
    for keyword in keywords:
        pattern = re.compile(rf'\b{re.escape(keyword)}\b', re.IGNORECASE)
        matches = pattern.findall(message)
        extracted.extend(matches)
    return extracted

df['extracted'] = df['Message'].apply(extract_matching_text)

df_filtered = df[df['extracted'].str.len() > 0]

output_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/event_final_df.csv'
df_filtered.to_csv(output_path, index=False)

print(f"Extracted data has been saved to {output_path}")

"""#Machine learning

##Event + Balanced
"""

file_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/event_final_df.csv'
df = pd.read_csv(file_path)

df['extracted'] = df['extracted'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

df_majority = df[df.Label == 0]
df_minority = df[df.Label == 1]

df_majority_downsampled = resample(df_majority,
                                   replace=False,
                                   n_samples=len(df_minority),
                                   random_state=42)

df_balanced = pd.concat([df_minority, df_majority_downsampled])

tfidf = TfidfVectorizer(max_features=1000)
X = tfidf.fit_transform(df_balanced['extracted'].apply(lambda x: ' '.join(eval(x)))).toarray()

y = df_balanced['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

models = {
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"\nResults for {model_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nDetailed Classification Report:")
    print(classification_report(y_test, y_pred))

"""##Event No Balance"""

file_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/event_final_df.csv'
df = pd.read_csv(file_path)

df['extracted'] = df['extracted'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

tfidf = TfidfVectorizer(max_features=1000)
X = tfidf.fit_transform(df['extracted'].apply(lambda x: ' '.join(eval(x)))).toarray()

y = df['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

models = {
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"\nResults for {model_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nDetailed Classification Report:")
    print(classification_report(y_test, y_pred))

"""##Perf ML

"""

file_path = '/content/drive/MyDrive/Master/R2_Machine_Learning/Final_Dataset/perf_dataset.csv'
df_data = pd.read_csv(file_path)

df_data['Open Ports Count'] = df_data['Open Ports'].apply(lambda x: len(str(x).split(',')))

X = df_data.drop(columns=['Timestamp', 'Label', 'Attack Technique', 'Open Ports' ,'Arp.Plc.Eclr/code.USER_PART_DT.MEM_FREE', 'Arp.Plc.Eclr/code.USER_PART_DT.MEM_USAGE', 'Arp.Plc.Eclr/code.USER_PART_DT.MEM_USED', 'Arp.Plc.Eclr/code.temp','Open Ports Count' ])
y = df_data['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

models = {
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"\nResults for {model_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nDetailed Classification Report:")
    print(classification_report(y_test, y_pred))

df_data['Open Ports Count'] = pd.to_numeric(df_data['Open Ports Count'], errors='coerce')
df_data['Arp.Plc.Eclr/code.temp'] = pd.to_numeric(df_data['Arp.Plc.Eclr/code.temp'], errors='coerce')

plt.figure(figsize=(12, 6))
sns.countplot(x='Open Ports Count', hue='Label', data=df_data)
plt.title('Count of Open Ports Count by Label (0 or 1)')
plt.xlabel('Open Ports Count')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(x='Arp.Plc.Eclr/code.temp', hue='Label', data=df_data)
plt.title('Count of Temperature by Label (0 or 1)')
plt.xlabel('Temperature (Â°C)')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.grid(True)
plt.show()